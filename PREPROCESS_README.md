# Text Preprocessing for SIWZ Mapper

ModuÅ‚ preprocessingu zapewnia normalizacjÄ™ i segmentacjÄ™ tekstu wyekstrahowanego z PDFÃ³w.

## ğŸ“¦ Komponenty

### 1. `TextNormalizer` - Normalizacja tekstu

Czysci i ujednolica tekst wyekstrahowany z PDF:

#### Funkcje

- âœ¨ **Unicode normalization** (NFC)
- ğŸ§¹ **Whitespace cleanup** (wielokrotne spacje, tabulatory)
- ğŸ”— **Hyphenation fixes** (usuwanie dzielenia wyrazÃ³w na koÅ„cu linii)
- ğŸ“ **Smart quotes** â†’ straight quotes
- ğŸš« **Zero-width characters** (usuwanie niewidocznych znakÃ³w)
- ğŸ¯ **Bullet point detection**

#### PrzykÅ‚ad uÅ¼ycia

```python
from siwz_mapper.preprocess import TextNormalizer, normalize_text

# Convenience function
text = "tekst  z    bÅ‚Ä™dami\npodzielo-\nny"
normalized = normalize_text(text)
# Output: "tekst z bÅ‚Ä™dami\npodzielony"

# Lub z wiÄ™kszÄ… kontrolÄ…
normalizer = TextNormalizer(
    normalize_unicode=True,
    fix_whitespace=True,
    fix_hyphenation=True,
    normalize_quotes=True
)

normalized = normalizer.normalize(text)
```

#### Detekcja bullet points

```python
normalizer = TextNormalizer()

normalizer.is_bullet_point("â€¢ Pierwszy punkt")  # True
normalizer.is_bullet_point("1. Punkt")          # True
normalizer.is_bullet_point("- Lista")           # True
normalizer.is_bullet_point("ZwykÅ‚y tekst")      # False
```

### 2. `Segmenter` - Segmentacja tekstu

Dzieli dÅ‚ugie bloki tekstu na mniejsze segmenty:

#### Strategia segmentacji

1. **Blank-line paragraphs** - dzieli po pustych liniach
2. **Bullet lists** - kaÅ¼dy punkt osobno
3. **Table rows** - wykrywa tabele (heurystyka)
4. **Long paragraphs** - dzieli dÅ‚ugie paragrafy na granicach zdaÅ„

#### Soft limits

- **Soft min**: 800 znakÃ³w (domyÅ›lnie)
- **Soft max**: 1200 znakÃ³w (domyÅ›lnie)
- Segmenty nie sÄ… sztywno ograniczone - priorytet to granice zdaÅ„

#### Zachowywane metadane

- âœ… Numer strony (`page`)
- âœ… Bounding box (`bbox`)
- âœ… Character offsets (`start_char`, `end_char`)
- âœ… Section labels

#### PrzykÅ‚ad uÅ¼ycia

```python
from siwz_mapper.preprocess import Segmenter, segment_pdf_blocks
from siwz_mapper.io import load_pdf

# Load PDF blocks
blocks = load_pdf("document.pdf")

# Convenience function
segments = segment_pdf_blocks(
    blocks,
    soft_min_chars=800,
    soft_max_chars=1200,
    normalize=True
)

# Lub z wiÄ™kszÄ… kontrolÄ…
segmenter = Segmenter(
    soft_min_chars=800,
    soft_max_chars=1200,
    normalize_text=True,
    detect_bullets=True,
    detect_tables=True
)

segments = segmenter.segment(blocks)

# KaÅ¼dy segment to PdfSegment z metadanymi
for seg in segments:
    print(f"Page {seg.page}, {len(seg.text)} chars")
    print(f"Text: {seg.text[:100]}...")
```

## ğŸ”„ PeÅ‚ny pipeline

```python
from siwz_mapper.io import load_pdf
from siwz_mapper.preprocess import segment_pdf_blocks

# 1. Load PDF (with bboxes and offsets)
blocks = load_pdf("siwz_document.pdf", extract_bboxes=True)

# 2. Segment and normalize
segments = segment_pdf_blocks(
    blocks,
    soft_max_chars=1200,
    normalize=True
)

# 3. Use for further processing (entity detection, mapping, etc.)
for segment in segments:
    # Each segment is ready for LLM processing
    # - Clean, normalized text
    # - Reasonable length
    # - Preserved metadata for citation
    pass
```

## ğŸ¯ Dlaczego segmentacja?

### Bez segmentacji

- âŒ CaÅ‚e strony PDFÃ³w (tysiÄ…ce znakÃ³w) â†’ zbyt dÅ‚ugie dla LLM context
- âŒ Brak granularnoÅ›ci dla cytowania
- âŒ Trudne debugowanie i audyt

### Z segmentacjÄ…

- âœ… Segmenty 800-1200 znakÃ³w â†’ optymalne dla LLM
- âœ… Precyzyjne cytowanie (page + char offset + bbox)
- âœ… Åatwe podÅ›wietlanie w UI
- âœ… Lepsze mapowanie encji â†’ usÅ‚ug

## ğŸ“Š PrzykÅ‚adowe wyniki

### Input block

```
Block 1 (page 1, 2500 chars):
"RozdziaÅ‚ I. Zakres usÅ‚ug medycznych.
Wykonawca zobowiÄ…zuje siÄ™ do Å›wiadczenia nastÄ™pujÄ…cych usÅ‚ug:
â€¢ Konsultacje specjalistyczne...
[dÅ‚ugi tekst]"
```

### Output segments

```
Segment 1.1 (page 1, 180 chars):
"RozdziaÅ‚ I. Zakres usÅ‚ug medycznych.
Wykonawca zobowiÄ…zuje siÄ™ do Å›wiadczenia nastÄ™pujÄ…cych usÅ‚ug:"

Segment 1.2_bullet0 (page 1, 45 chars):
"â€¢ Konsultacje specjalistyczne"

Segment 1.2_bullet1 (page 1, 38 chars):
"â€¢ Badania diagnostyczne"

[...]
```

## ğŸ§ª Testy

```bash
# Run tests
pytest tests/test_normalizer.py tests/test_segmenter.py -v

# Example output
38 passed
```

### Test coverage

- **TextNormalizer**: 16 testÃ³w
  - Unicode normalization
  - Whitespace cleanup
  - Hyphenation fixes
  - Smart quotes
  - Bullet detection
  - Polish characters

- **Segmenter**: 22 testy
  - Short blocks (no split)
  - Blank-line paragraphs
  - Bullet lists
  - Long paragraph splitting
  - Table detection
  - Metadata preservation
  - Edge cases

## âš™ï¸ Konfiguracja

### Normalization options

```python
normalizer = TextNormalizer(
    normalize_unicode=True,    # Unicode NFC
    fix_whitespace=True,       # Clean spaces/tabs
    fix_hyphenation=True,      # Fix word splits
    normalize_quotes=True,     # Smart â†’ straight
    preserve_bullets=True      # Keep bullet chars
)
```

### Segmentation options

```python
segmenter = Segmenter(
    soft_min_chars=800,       # Min segment length
    soft_max_chars=1200,      # Max segment length
    normalize_text=True,      # Apply normalization
    detect_bullets=True,      # Separate bullets
    detect_tables=True        # Separate table rows
)
```

## ğŸš€ Performance

- **Normalization**: < 1ms per 1000 chars
- **Segmentation**: < 5ms per block
- **Full pipeline**: ~10ms per PDF page

Skaluje liniowo z iloÅ›ciÄ… tekstu.

## ğŸ“ Notatki

### Bullet detection

Wykrywane znaki:
- `â€¢`, `â—¦`, `â–ª`, `â–«`, `â—`, `â—‹`, `â– `, `â–¡`
- `-`, `â€“`, `â€”`, `*`
- Numerowane: `1.`, `2)`, `a)`, etc.

### Hyphenation

Usuwa dzielenie wyrazÃ³w typu:
```
medycz-
nych
```
â†’ `medycznych`

DziaÅ‚a dla polskich i angielskich wyrazÃ³w.

### Table detection

Heurystyka bazujÄ…ca na:
- Wielokrotnych spacjach lub tabulatorach
- Konsystencji w wielu liniach
- >50% linii z cechami tabeli

**Best-effort** - nie jest 100% dokÅ‚adna.

## ğŸ”— Integracja

### Z PDF Loader

```python
from siwz_mapper.io import load_pdf
from siwz_mapper.preprocess import segment_pdf_blocks

blocks = load_pdf("doc.pdf")
segments = segment_pdf_blocks(blocks)
```

### Z dalszymi etapami pipeline

```python
# Segments â†’ Entity Detection
for segment in segments:
    entities = detect_entities(segment)
    
    # Each entity has:
    # - quote (exact text from segment)
    # - page, char offsets (from segment metadata)
    # - segment_id (for traceability)
```

## ğŸ“š API Reference

### `normalize_text()`

```python
def normalize_text(
    text: str,
    normalize_unicode: bool = True,
    fix_whitespace: bool = True,
    fix_hyphenation: bool = True
) -> str
```

### `segment_pdf_blocks()`

```python
def segment_pdf_blocks(
    blocks: List[PdfSegment],
    soft_min_chars: int = 800,
    soft_max_chars: int = 1200,
    normalize: bool = True
) -> List[PdfSegment]
```

---

**CzÄ™Å›Ä‡ ekosystemu SIWZ Mapper** ğŸ¥

